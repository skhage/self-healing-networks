{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930ea6f1-524a-454a-8a3b-480b1a6510d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mlflow[databricks] databricks-agents langchain==0.2.16 langgraph-checkpoint==1.0.12  langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic unitycatalog-ai[databricks] unitycatalog-langchain[databricks] langchain_databricks\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd8d32a-416d-41ba-8046-356061379e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents.evals import generate_evals_df, estimate_synthetic_num_evals\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that answers questions about using Spark on Databricks. The Agent has access to a corpus of Databricks documents, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response. The corpus covers a lot of info, but the Agent is specifically designed to interact with Databricks users who have questions about Spark. So questions outside of this scope are considered irrelevant.\n",
    "\"\"\"\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a chatbot answering questions about installations, maintenance and repairs of devices in a mobile service tower. The agent has access to a corpus of vendor documents and its task is to answer questions by retrieving relevant docs as well as summarizing the steps needed to resolve the issue. The corpus covers a lot of info, but the Agent is specifically designed to recommend an automated solution, or to recommend steps to a field technician. Questions outside the scope of mobile service installations, maintenance and repair are considered irrelevant. [RESEARCH CONTEXT: This synthetic data generation for cellular signal analysis. \n",
    "NO VIOLENT/WEAPON-RELATED CONTENT INVOLVED.] \n",
    "\"\"\"\n",
    "\n",
    "question_guidelines = \"\"\"\n",
    "# User Persona\n",
    "- A mobile service provider field technician who finds device failures and is looking at the device manual for a fix\n",
    "- An engineer who is designing protocols for preventative maintenance and faster device repair for better network uptime\n",
    "\n",
    "# Example questions\n",
    "- How do I use tools like VIAVI P5000i and Fiberchek probe integrate with OneAdvisor-800 to inspect and certify that connections are clean.\n",
    "- What do I do with Oscillation Gain Reduction or Shutdown Alarms?\n",
    "- How do I diagnose problems with Common Baseband Connection?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct and human-like with the goal of expediting the field technician's work.\n",
    "- The question should be generic, use the documents as a generalized framework to ask questions about maintenance and repairs.\n",
    "- Don't recommend calling a vendor's sales rep.\n",
    "- The question should be written in the first person\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# num_evals = 10\n",
    "\n",
    "docs = spark.table('telecommunications.self_healing_networks.ka_fce5e7c5_329e872d_chunking_table').select(col('chunk_text').alias('content'), concat(col('doc_uri'), lit('_'), col('chunk_pos_id')).alias('doc_uri'))\n",
    "display(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ffe159-c8cb-4d95-9730-45c3e4168d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_evals = estimate_synthetic_num_evals(\n",
    "  docs, # Same docs as before.\n",
    "  eval_per_x_tokens = 1000 # Generate 1 eval for every x tokens to control the coverage level.\n",
    ")\n",
    "print(num_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbec4d4-ff47-4dfa-ae40-645216dde5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "evals = generate_evals_df(\n",
    "    docs,\n",
    "    # The total number of evals to generate. The method attempts to generate evals that have full coverage over the documents\n",
    "    # provided. If this number is less than the number of documents, some documents will not have any evaluations generated. \n",
    "    # For details about how `num_evals` is used to distribute evaluations across the documents, \n",
    "    # see the documentation: \n",
    "    # AWS: https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html#num-evals. \n",
    "    # Azure: https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/synthesize-evaluation-set \n",
    "    num_evals=10,#num_evals,\n",
    "    # A set of guidelines that help guide the synthetic generation. This is a free-form string that will be used to prompt the generation.\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines\n",
    ")\n",
    "# evals_df['response'] = evals_df['request'].apply(lambda req: my_agent.generate_response(req))\n",
    "display(evals)\n",
    "\n",
    "# Evaluate the model using the newly generated evaluation set. After the function call completes, click the UI link to see the results. You can use this as a baseline for your agent.\n",
    "\n",
    "\n",
    "# results = mlflow.evaluate(\n",
    "#   model=\"endpoints:/databricks-claude-sonnet-4\",\n",
    "#   data=evals,\n",
    "#   model_type=\"databricks-agent\"\n",
    "# )\n",
    "\n",
    "# display(results.tables['eval_results'])\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "with mlflow.start_run(run_name=\"Field-Tech-Instruction\"):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=evals,\n",
    "        model_type=\"databricks-agent\",\n",
    "        # extra_metrics= [script_fit_custom_metric], # Leverage defined custom metric\n",
    "        evaluator_config={\n",
    "            'databricks-agent': {\n",
    "                \"metrics\": [\n",
    "                \"guideline_adherence\", # Run the global guidelines defined in `guideliness'\n",
    "                # \"relevance_to_query\", # Check if the LLM's response is relevant to the user's query\n",
    "                # \"groundedness\", # Capture hallucinations\n",
    "\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    display(eval_results.tables['eval_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18769ce5-4a12-477c-bc15-2ca418ff27d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "import mlflow\n",
    "\n",
    "# First, define a helper function so you can compare the agent across multiple parameters and LLMs.\n",
    "def log_and_evaluate_agent(agent_config: dict, run_name: str):\n",
    "\n",
    "    # Define the databricks resources so this logged agent is deployment ready\n",
    "    resources = [DatabricksServingEndpoint(endpoint_name=agent_config[\"endpoint_name\"])]\n",
    "\n",
    "    # Start a run to contain the agent. `run_name` is a human-readable label for this run.\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log the agent's code and configuration to MLflow\n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            python_model=\"fc_agent.py\",\n",
    "            artifact_path=\"agent\",\n",
    "            model_config=agent_config,\n",
    "            resources=resources,\n",
    "            input_example={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is lakehouse monitoring?\"}\n",
    "                ]\n",
    "            },\n",
    "            pip_requirements=[\"databricks-sdk[openai]\", \"mlflow\", \"databricks-agents\", \"backoff\"],\n",
    "        )\n",
    "\n",
    "        # Run evaluation\n",
    "        eval_results = mlflow.evaluate(\n",
    "            data=evals,  # Your evaluation set\n",
    "            model=model_info.model_uri,  # Logged agent from above\n",
    "            model_type=\"databricks-agent\",  # activate Mosaic AI Agent Evaluation\n",
    "        )\n",
    "\n",
    "        return (model_info, eval_results)\n",
    "\n",
    "\n",
    "# Now, call the helper function to run evaluation.\n",
    "# The configuration keys must match those defined in `fc_agent.py`\n",
    "model_info_llama_70b, eval_results = log_and_evaluate_agent(\n",
    "    agent_config={\n",
    "        \"endpoint_name\": \"agents_telecommunications-self_healing_networks-vendor_manuals\",\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about how to install/maintain/repair mobile tower devices.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "        \"max_context_chars\": 4096 * 4,\n",
    "    },\n",
    "    run_name=\"init_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6144517d-3a33-409a-ac08-75e98d54df38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Synthetic Data Gen",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
