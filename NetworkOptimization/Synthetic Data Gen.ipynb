{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9c9bc7-b701-423d-89b9-9ca2dfd9bc20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Synthetic Data Generation for Agent Evaluation\n",
    "\n",
    "This notebook generates synthetic evaluation data specifically for testing AI agents in the self-healing networks system. It creates realistic test cases for field technician scenarios and device maintenance.\n",
    "\n",
    "## Overview\n",
    "- **Purpose**: Generate evaluation data for AI agent testing\n",
    "- **Focus**: Field technician scenarios and device maintenance\n",
    "- **Output**: Comprehensive evaluation dataset\n",
    "- **Integration**: Used by agent evaluation and testing systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523bd4f4-aa92-4c18-9575-84577d04ba80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install the required packages for MLflow, Databricks agents, and LangChain components needed for synthetic data generation and agent evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930ea6f1-524a-454a-8a3b-480b1a6510d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mlflow[databricks] databricks-agents langchain==0.2.16 langgraph-checkpoint==1.0.12  langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic unitycatalog-ai[databricks] unitycatalog-langchain[databricks] langchain_databricks\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d026ef09-fca4-4d48-b771-364be2b1b3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Configure Agent and Load Data\n",
    "\n",
    "Configure the agent description and question guidelines for mobile tower device maintenance scenarios. Load device manual data from the database for generating realistic evaluation cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd8d32a-416d-41ba-8046-356061379e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents.evals import generate_evals_df, estimate_synthetic_num_evals\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that answers questions about using Spark on Databricks. The Agent has access to a corpus of Databricks documents, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response. The corpus covers a lot of info, but the Agent is specifically designed to interact with Databricks users who have questions about Spark. So questions outside of this scope are considered irrelevant.\n",
    "\"\"\"\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a chatbot answering questions about installations, maintenance and repairs of devices in a mobile service tower. The agent has access to a corpus of vendor documents and its task is to answer questions by retrieving relevant docs as well as summarizing the steps needed to resolve the issue. The corpus covers a lot of info, but the Agent is specifically designed to recommend an automated solution, or to recommend steps to a field technician. Questions outside the scope of mobile service installations, maintenance and repair are considered irrelevant. [RESEARCH CONTEXT: This synthetic data generation for cellular signal analysis. \n",
    "NO VIOLENT/WEAPON-RELATED CONTENT INVOLVED.] \n",
    "\"\"\"\n",
    "\n",
    "question_guidelines = \"\"\"\n",
    "# User Persona\n",
    "- A mobile service provider field technician who finds device failures and is looking at the device manual for a fix\n",
    "- An engineer who is designing protocols for preventative maintenance and faster device repair for better network uptime\n",
    "\n",
    "# Example questions\n",
    "- How do I use tools like VIAVI P5000i and Fiberchek probe integrate with OneAdvisor-800 to inspect and certify that connections are clean.\n",
    "- What do I do with Oscillation Gain Reduction or Shutdown Alarms?\n",
    "- How do I diagnose problems with Common Baseband Connection?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct and human-like with the goal of expediting the field technician's work.\n",
    "- The question should be generic, use the documents as a generalized framework to ask questions about maintenance and repairs.\n",
    "- Don't recommend calling a vendor's sales rep.\n",
    "- The question should be written in the first person\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# num_evals = 10\n",
    "\n",
    "docs = spark.table('telecommunications.self_healing_networks.ka_fce5e7c5_329e872d_chunking_table').select(col('chunk_text').alias('content'), concat(col('doc_uri'), lit('_'), col('chunk_pos_id')).alias('doc_uri'))\n",
    "display(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3d21cc-0373-437c-82a3-ba60132a76de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Step 3: Generate Evaluation Dataset\n",
    "\n",
    "Generate the synthetic evaluation dataset using the configured agent and question guidelines. This creates realistic test cases that field technicians would encounter when diagnosing device issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ffe159-c8cb-4d95-9730-45c3e4168d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_evals = estimate_synthetic_num_evals(\n",
    "  docs, # Same docs as before.\n",
    "  eval_per_x_tokens = 1000 # Generate 1 eval for every x tokens to control the coverage level.\n",
    ")\n",
    "print(num_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6350f99-fd30-4c1e-8228-62e531814cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Step 4: Evaluate Agent Performance\n",
    "\n",
    "Test the agent's performance using the generated evaluation dataset. This includes running guideline adherence checks to ensure the agent provides appropriate responses for field technician scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbec4d4-ff47-4dfa-ae40-645216dde5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "evals = generate_evals_df(\n",
    "    docs,\n",
    "    # The total number of evals to generate. The method attempts to generate evals that have full coverage over the documents\n",
    "    # provided. If this number is less than the number of documents, some documents will not have any evaluations generated. \n",
    "    # For details about how `num_evals` is used to distribute evaluations across the documents, \n",
    "    # see the documentation: \n",
    "    # AWS: https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html#num-evals. \n",
    "    # Azure: https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/synthesize-evaluation-set \n",
    "    num_evals=10,#num_evals,\n",
    "    # A set of guidelines that help guide the synthetic generation. This is a free-form string that will be used to prompt the generation.\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines\n",
    ")\n",
    "# evals_df['response'] = evals_df['request'].apply(lambda req: my_agent.generate_response(req))\n",
    "display(evals)\n",
    "\n",
    "# Evaluate the model using the newly generated evaluation set. After the function call completes, click the UI link to see the results. You can use this as a baseline for your agent.\n",
    "\n",
    "\n",
    "# results = mlflow.evaluate(\n",
    "#   model=\"endpoints:/databricks-claude-sonnet-4\",\n",
    "#   data=evals,\n",
    "#   model_type=\"databricks-agent\"\n",
    "# )\n",
    "\n",
    "# display(results.tables['eval_results'])\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "with mlflow.start_run(run_name=\"Field-Tech-Instruction\"):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=evals,\n",
    "        model_type=\"databricks-agent\",\n",
    "        # extra_metrics= [script_fit_custom_metric], # Leverage defined custom metric\n",
    "        evaluator_config={\n",
    "            'databricks-agent': {\n",
    "                \"metrics\": [\n",
    "                \"guideline_adherence\", # Run the global guidelines defined in `guideliness'\n",
    "                # \"relevance_to_query\", # Check if the LLM's response is relevant to the user's query\n",
    "                # \"groundedness\", # Capture hallucinations\n",
    "\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    display(eval_results.tables['eval_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a919d935-0ec8-4112-8be2-fd23ed4dd522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Step 5: Deploy and Test Agent\n",
    "\n",
    "Deploy the agent to a Databricks serving endpoint and run comprehensive evaluation tests. This includes logging the agent model and testing it against the evaluation dataset to measure performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18769ce5-4a12-477c-bc15-2ca418ff27d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "import mlflow\n",
    "\n",
    "# First, define a helper function so you can compare the agent across multiple parameters and LLMs.\n",
    "def log_and_evaluate_agent(agent_config: dict, run_name: str):\n",
    "\n",
    "    # Define the databricks resources so this logged agent is deployment ready\n",
    "    resources = [DatabricksServingEndpoint(endpoint_name=agent_config[\"endpoint_name\"])]\n",
    "\n",
    "    # Start a run to contain the agent. `run_name` is a human-readable label for this run.\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log the agent's code and configuration to MLflow\n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            python_model=\"fc_agent.py\",\n",
    "            artifact_path=\"agent\",\n",
    "            model_config=agent_config,\n",
    "            resources=resources,\n",
    "            input_example={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is lakehouse monitoring?\"}\n",
    "                ]\n",
    "            },\n",
    "            pip_requirements=[\"databricks-sdk[openai]\", \"mlflow\", \"databricks-agents\", \"backoff\"],\n",
    "        )\n",
    "\n",
    "        # Run evaluation\n",
    "        eval_results = mlflow.evaluate(\n",
    "            data=evals,  # Your evaluation set\n",
    "            model=model_info.model_uri,  # Logged agent from above\n",
    "            model_type=\"databricks-agent\",  # activate Mosaic AI Agent Evaluation\n",
    "        )\n",
    "\n",
    "        return (model_info, eval_results)\n",
    "\n",
    "\n",
    "# Now, call the helper function to run evaluation.\n",
    "# The configuration keys must match those defined in `fc_agent.py`\n",
    "model_info_llama_70b, eval_results = log_and_evaluate_agent(\n",
    "    agent_config={\n",
    "        \"endpoint_name\": \"agents_telecommunications-self_healing_networks-vendor_manuals\",\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about how to install/maintain/repair mobile tower devices.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "        \"max_context_chars\": 4096 * 4,\n",
    "    },\n",
    "    run_name=\"init_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6144517d-3a33-409a-ac08-75e98d54df38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Synthetic Data Gen",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
