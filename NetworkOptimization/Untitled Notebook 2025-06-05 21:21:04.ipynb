{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb516afa-33b9-4cd6-ad38-77811ca70877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-agents mlflow databricks-sdk[openai] backoff\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5115d6de-0d52-43a3-a783-52343933df9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile fc_agent.py\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import pandas as pd\n",
    "from typing import Any, Union, Dict, List, Optional\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ChatModel\n",
    "from mlflow.types.llm import ChatCompletionResponse, ChatMessage, ChatParams, ChatChoice\n",
    "from dataclasses import asdict\n",
    "import dataclasses\n",
    "import json\n",
    "import backoff  # for exponential backoff on LLM rate limits\n",
    "\n",
    "\n",
    "# Default configuration for the agent.\n",
    "DEFAULT_CONFIG = {\n",
    "    'endpoint_name': \"databricks-claude-sonnet-4\",\n",
    "    'temperature': 0.01,\n",
    "    'max_tokens': 1000,\n",
    "    'system_prompt': \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "    'max_context_chars': 4096 * 4\n",
    "}\n",
    "\n",
    "# OpenAI-formatted function for the retriever tool\n",
    "RETRIEVER_TOOL_SPEC = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_product_docs\",\n",
    "        \"description\": \"Use this tool to search for Databricks product documentation.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"description\": \"a set of individual keywords to find relevant docs for. each item of the array must be a single word.\",\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "\n",
    "class FunctionCallingAgent(mlflow.pyfunc.ChatModel):\n",
    "    \"\"\"\n",
    "    Class representing a function-calling agent that has one tool: a retriever using keyword-based search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the OpenAI SDK client connected to Model Serving.\n",
    "        Load the agent's configuration from MLflow Model Config.\n",
    "        \"\"\"\n",
    "        # Initialize OpenAI SDK connected to Model Serving\n",
    "        w = WorkspaceClient()\n",
    "        self.model_serving_client: OpenAI = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "        # Load config\n",
    "        # When this agent is deployed to Model Serving, the configuration loaded here is replaced with the config passed to mlflow.pyfunc.log_model(model_config=...)\n",
    "        self.config = mlflow.models.ModelConfig(development_config=DEFAULT_CONFIG)\n",
    "\n",
    "        # Configure playground, review app, and agent evaluation to display the chunks from the retriever \n",
    "        mlflow.models.set_retriever_schema(\n",
    "            name=\"db_docs\",\n",
    "            primary_key=\"chunk_id\",\n",
    "            text_column=\"chunked_text\",\n",
    "            doc_uri=\"doc_uri\",\n",
    "        )\n",
    "\n",
    "        # Load the retriever tool's docs.\n",
    "        raw_docs_parquet = \"https://github.com/databricks/genai-cookbook/raw/refs/heads/main/quick_start_demo/chunked_databricks_docs.snappy.parquet\"\n",
    "        self.docs = pd.read_parquet(raw_docs_parquet).to_dict(\"records\")\n",
    "\n",
    "        # Identify the function used as the retriever tool\n",
    "        self.tool_functions = {\n",
    "            'search_product_docs': self.search_product_docs\n",
    "        }\n",
    "\n",
    "    @mlflow.trace(name=\"rag_agent\", span_type=\"AGENT\")\n",
    "    def predict(\n",
    "        self, context=None, messages: List[ChatMessage]=None, params: Optional[ChatParams] = None\n",
    "    ) -> ChatCompletionResponse:\n",
    "        \"\"\"\n",
    "        Primary function that takes a user's request and generates a response.\n",
    "        \"\"\"\n",
    "        if messages is None:\n",
    "            raise ValueError(\"predict(...) called without `messages` parameter.\")\n",
    "        \n",
    "        # Convert all input messages to dict from ChatMessage\n",
    "        messages = convert_chat_messages_to_dict(messages)\n",
    "\n",
    "        # Add system prompt\n",
    "        request = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": self.config.get('system_prompt')},\n",
    "                    *messages,\n",
    "                ],\n",
    "            }\n",
    "            \n",
    "        # Ask the LLM to call tools and generate the response\n",
    "        output= self.recursively_call_and_run_tools(\n",
    "            **request\n",
    "        )\n",
    "        \n",
    "        # Convert response to ChatCompletionResponse dataclass\n",
    "        return ChatCompletionResponse.from_dict(output)\n",
    "    \n",
    "    @mlflow.trace(span_type=\"RETRIEVER\")\n",
    "    def search_product_docs(self, query: list[str]) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Retriever tool. Simple keyword-based retriever - would be replaced with a Vector Index\n",
    "        \"\"\"\n",
    "        keywords = query\n",
    "        if len(keywords) == 0:\n",
    "            return []\n",
    "        result = []\n",
    "        for chunk in self.docs:\n",
    "            score = sum(\n",
    "                (keyword.lower() in chunk[\"chunked_text\"].lower())\n",
    "                for keyword in keywords\n",
    "            )\n",
    "            result.append(\n",
    "                {\n",
    "                    \"page_content\": chunk[\"chunked_text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"doc_uri\": chunk[\"url\"],\n",
    "                        \"score\": score,\n",
    "                        \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        ranked_docs = sorted(result, key=lambda x: x[\"metadata\"][\"score\"], reverse=True)\n",
    "        cutoff_docs = []\n",
    "        context_budget_left = self.config.get(\"max_context_chars\")\n",
    "        for doc in ranked_docs:\n",
    "            content = doc[\"page_content\"]\n",
    "            doc_len = len(content)\n",
    "            if context_budget_left < doc_len:\n",
    "                cutoff_docs.append(\n",
    "                    {**doc, \"page_content\": content[:context_budget_left]}\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                cutoff_docs.append(doc)\n",
    "            context_budget_left -= doc_len\n",
    "        return cutoff_docs\n",
    "\n",
    "    ##\n",
    "    # Helper functions below\n",
    "    ##\n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    def completions_with_backoff(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper: exponetially backoff if the LLM's rate limit is exceeded.\n",
    "        \"\"\"\n",
    "        traced_chat_completions_create_fn = mlflow.trace(\n",
    "            self.model_serving_client.chat.completions.create,\n",
    "            name=\"chat_completions_api\",\n",
    "            span_type=\"CHAT_MODEL\",\n",
    "        )\n",
    "        return traced_chat_completions_create_fn(**kwargs)\n",
    "\n",
    "    def chat_completion(self, messages: List[ChatMessage]) -> ChatCompletionResponse:\n",
    "        \"\"\"\n",
    "        Helper: Call the LLM configured via the ModelConfig using the OpenAI SDK\n",
    "        \"\"\"\n",
    "        request = {\"messages\": messages, \"temperature\": self.config.get(\"temperature\"), \"max_tokens\": self.config.get(\"max_tokens\"),  \"tools\": RETRIEVER_TOOL_SPEC}\n",
    "        return self.completions_with_backoff(\n",
    "            model=self.config.get(\"endpoint_name\"), **request,\n",
    "                \n",
    "        )\n",
    "\n",
    "    @mlflow.trace(span_type=\"CHAIN\")\n",
    "    def recursively_call_and_run_tools(self, max_iter=10, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper: Recursively calls the LLM using the tools in the prompt. Either executes the tools and recalls the LLM or returns the LLM's generation.\n",
    "        \"\"\"\n",
    "        messages = kwargs[\"messages\"]\n",
    "        del kwargs[\"messages\"]\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            with mlflow.start_span(name=f\"iteration_{i}\", span_type=\"CHAIN\") as span:\n",
    "                response = self.chat_completion(messages=messages)\n",
    "                assistant_message = response.choices[0].message  # openai client\n",
    "                tool_calls = assistant_message.tool_calls  # openai\n",
    "                if tool_calls is None:\n",
    "                    # the tool execution finished, and we have a generation\n",
    "                    return response.to_dict()\n",
    "                tool_messages = []\n",
    "                for tool_call in tool_calls:  # TODO: should run in parallel\n",
    "                    with mlflow.start_span(\n",
    "                        name=\"execute_tool\", span_type=\"TOOL\"\n",
    "                    ) as span:\n",
    "                        function = tool_call.function  \n",
    "                        args = json.loads(function.arguments)  \n",
    "                        span.set_inputs(\n",
    "                            {\n",
    "                                \"function_name\": function.name,\n",
    "                                \"function_args_raw\": function.arguments,\n",
    "                                \"function_args_loaded\": args,\n",
    "                            }\n",
    "                        )\n",
    "                        result = self.execute_function(\n",
    "                            self.tool_functions[function.name], args\n",
    "                        )\n",
    "                        tool_message = {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": result,\n",
    "                        } \n",
    "\n",
    "                        tool_messages.append(tool_message)\n",
    "                        span.set_outputs({\"new_message\": tool_message})\n",
    "                assistant_message_dict = assistant_message.dict().copy()  \n",
    "                del assistant_message_dict[\"content\"]\n",
    "                del assistant_message_dict[\"function_call\"] \n",
    "                if \"audio\" in assistant_message_dict:\n",
    "                    del assistant_message_dict[\"audio\"]  # hack to make llama70b work\n",
    "                messages = (\n",
    "                    messages\n",
    "                    + [\n",
    "                        assistant_message_dict,\n",
    "                    ]\n",
    "                    + tool_messages\n",
    "                )\n",
    "                i += 1\n",
    "        # TODO: Handle more gracefully\n",
    "        raise \"ERROR: max iter reached\"\n",
    "\n",
    "    def execute_function(self, tool, args):\n",
    "        \"\"\"\n",
    "        Execute a tool and return the result as a JSON string\n",
    "        \"\"\"\n",
    "        result = tool(**args)\n",
    "        return json.dumps(result)\n",
    "        \n",
    "def convert_chat_messages_to_dict(messages: List[ChatMessage]):\n",
    "    new_messages = []\n",
    "    for message in messages:\n",
    "        if type(message) == ChatMessage:\n",
    "            # Remove any keys with None values\n",
    "            new_messages.append({k: v for k, v in asdict(message).items() if v is not None})\n",
    "        else:\n",
    "            new_messages.append(message)\n",
    "    return new_messages\n",
    "    \n",
    "\n",
    "# tell MLflow logging where to find the agent's code\n",
    "mlflow.models.set_model(FunctionCallingAgent())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-05 21:21:04",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
