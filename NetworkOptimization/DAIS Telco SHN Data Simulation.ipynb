{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d106fe07-1ffe-4fd5-ad66-83f9ae8fff31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea22025-ad1f-43a1-8322-9fbafd18d263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('params.yml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "data_params = params.get('data_params')\n",
    "CATALOG = data_params.get('catalog')\n",
    "SCHEMA = data_params.get('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033a45d4-00fe-41f2-89a2-550c5a2ea998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "colnames = ['radio', 'mcc', 'net', 'area', 'cell', 'unit', 'lon', 'lat', 'range', 'samples', 'changeable', 'created', 'updated', 'averageSignal']\n",
    "\n",
    "mobilelocations = pd.read_csv(f\"/Volumes/{CATALOG}/{SCHEMA}/{data_params.get('towers_volume')}/Tower Locations.csv\", names=colnames)\n",
    "\n",
    "spark.createDataFrame(mobilelocations).write.saveAsTable(f\"{CATALOG}.{SCHEMA}.mobilelocations\", mode=\"overwrite\")\n",
    "for col, comment in zip(colnames, [\n",
    "    \"Network type. One of the strings GSM, UMTS, LTE or CDMA.\",\n",
    "    \"Mobile Country Code, for example 260 for Poland.\",\n",
    "    \"For GSM, UMTS and LTE networks, this is the Mobile Network Code (MNC). For CDMA networks, this is the System IDentification number (SID).\",\n",
    "    \"Location Area Code (LAC) for GSM and UMTS networks. Tracking Area Code (TAC) for LTE networks. Network IDenfitication number (NID) for CDMA networks.\",\n",
    "    \"Cell ID (CID) for GSM and LTE networks. UTRAN Cell ID / LCID for UMTS networks, which is the concatenation of 2 or 4 bytes of Radio Network Controller (RNC) code and 4 bytes of Cell ID. Base station IDentifier number (BID) for CDMA networks.\",\n",
    "    \"Primary Scrambling Code (PSC) for UMTS networks. Physical Cell ID (PCI) for LTE networks. An empty value for GSM and CDMA networks.\",\n",
    "    \"Longitude in degrees between -180.0 and 180.0 changeable=1: average of longitude values of all related measurements changeable=0: exact GPS position of the cell tower\",\n",
    "    \"Latitude in degrees between -90.0 and 90.0 changeable=1: average of latitude values of all related measurements changeable=0: exact GPS position of the tower\",\n",
    "    \"Estimate of cell range, in meters.\",\n",
    "    \"Total number of measurements assigned to the cell tower\",\n",
    "    \"Defines if coordinates of the cell tower are exact or approximate. changeable=1: the GPS position of the cell tower has been calculated from all available measurements changeable=0: the GPS position of the cell tower is precise - no measurements have been used to calculate it.\",\n",
    "    \"The first time when the cell tower was seen and added to the OpenCellID database. A date in timestamp format: number of seconds since the UTC Unix Epoch of 1970-01-01T00:00:00Z For example 1409522613 is the timestamp for 2014-08-31T22:03:33Z.\",\n",
    "    \"The last time when the cell tower was seen and update. A date in timestamp format: number of seconds since the UTC Unix Epoch of 1970-01-01T00:00:00Z For example 1409522613 is the timestamp for 2014-08-31T22:03:33Z.\",\n",
    "    \"Average signal strength.\"\n",
    "]):\n",
    "    spark.sql(f\"ALTER TABLE {CATALOG}.{SCHEMA}.mobilelocations CHANGE COLUMN {col} COMMENT '{comment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5610f75-72f0-4eae-a71a-2005a9dd09b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_unixtime\n",
    "\n",
    "mobilelocations = (spark.table(f\"{CATALOG}.{SCHEMA}.mobilelocations\")\n",
    "                    .withColumn(\"created\", from_unixtime(col(\"created\")).cast(\"timestamp\"))\n",
    "                    .withColumn(\"updated\", from_unixtime(col(\"updated\")).cast(\"timestamp\")))\n",
    "\n",
    "\n",
    "sf_mobilelocations = mobilelocations.filter(\n",
    "    (mobilelocations.lat >= 37.6) & (mobilelocations.lat <= 37.9) &\n",
    "    (mobilelocations.lon >= -123.0) & (mobilelocations.lon <= -122.3) &\n",
    "    (mobilelocations.radio == 'LTE')\n",
    ").withColumn(\"tower_id\", expr(\"uuid()\"))\n",
    "\n",
    "\n",
    "sf_mobilelocations.write.option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.sf_mobilelocations\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4665ea8c-adfc-45b1-b176-3710866ebd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Field Technician Data Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6dc185-1888-4720-b0b0-5cb72421d4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate data for field technicians\n",
    "tech_data = [\n",
    "    Row(tech_id=fake.uuid4(), name=fake.name(), truckid=random.randint(1000, 9999))\n",
    "    for _ in range(params.get('simulation_params').get('n_fieldtechs'))\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of Rows\n",
    "field_technicians = spark.createDataFrame(tech_data)\n",
    "field_technicians.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.field_technicians\")\n",
    "display(field_technicians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b33664-7b25-49e3-8a13-2edf42988036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def choose_scenario():\n",
    "    import yaml\n",
    "    import random\n",
    "    with open('/Workspace/Shared/DAIS_2025_Demos/Telco/NetworkOptimization/params.yml', 'r') as file:\n",
    "        params = yaml.safe_load(file)\n",
    "    scenarios = params.get('Scenarios')\n",
    "    return random.choice(scenarios)\n",
    "\n",
    "choose_scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232c068f-c4ee-4e23-9a5d-7d89d3338c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scenarios = params.get(\"Scenarios\")\n",
    "scenarios_df = spark.createDataFrame([(scenario,) for scenario in scenarios], [\"scenarios\"])\n",
    "scenarios_df.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.scenarios\")\n",
    "display(scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e1dbfb-cb2d-47ce-86b9-65350ee0919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devicelist = ['Nokia AirScale Baseband Unit', 'Ericsson RBS 2106', 'Ericsson RRH 6000', 'Ericsson AIR Antenna 5331', 'Delta D750 DC Power System', 'Delta DPR 4000B EnergE', 'Generac MDG25IF4-STD3', 'Swift Sensors SS3-301 Wireless Door Sensor', 'Cisco Meraki MT10 Sensor', 'Cisco Catalyst 2960', 'Cisco ISR 4000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06578864-0010-45ec-a172-924505747a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, explode, expr\n",
    "towers = spark.table(f\"{CATALOG}.{SCHEMA}.sf_mobilelocations\").withColumn('devices', lit(devicelist))\n",
    "\n",
    "towers = towers.withColumn('device', explode('devices')).drop('devices').withColumn('device_id', expr(\"uuid()\"))\n",
    "towers.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.tower_devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffac4209-477a-4e51-b014-ba0bd3b53068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(towers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40f087e-513f-41d8-baf2-a72e1f52b6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sequence, to_date, expr\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "import random\n",
    "\n",
    "# Define the schema for the simulated data\n",
    "schema = StructType([\n",
    "    StructField(\"tower_id\", IntegerType(), False),\n",
    "    StructField(\"device_name\", StringType(), False),\n",
    "    StructField(\"outage_detected_date\", StringType(), False),\n",
    "    StructField(\"outage_resolved_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Generate a list of dates for 5 years\n",
    "start_date = \"2020-06-06\"\n",
    "end_date = \"2025-06-06\"\n",
    "date_seq = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\").selectExpr(\"explode(date_seq) as date\")\n",
    "\n",
    "# Simulate data\n",
    "data = []\n",
    "for row in date_seq.collect():\n",
    "    date = row['date']\n",
    "    for tower_id in range(1, 11):  # Assuming 10 towers\n",
    "        for device_id in range(1, 6):  # Assuming 5 devices per tower\n",
    "            outage_detected_date = date\n",
    "            outage_resolved_date = date + expr(\"INTERVAL 1 DAY\")\n",
    "            data.append((tower_id, f\"device_{device_id}\", str(outage_detected_date), str(outage_resolved_date)))\n",
    "\n",
    "# Create DataFrame\n",
    "simulated_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Display the simulated data\n",
    "display(simulated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2562ed66-11b8-416e-800f-1212d38c555e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "towers = spark.table(f\"{CATALOG}.{SCHEMA}.tower_devices\")\n",
    "# tower_ids = towers.select('tower_id').toPandas()['tower_id'].to_list()\n",
    "# device_ids = towers.select('device_id').toPandas()['device_id'].to_list()\n",
    "# num_towers = towers.select('device_id').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create device dataframe\n",
    "device_df = towers.select(['tower_id', 'device_id']).toPandas()\n",
    "\n",
    "# Generate outage data\n",
    "start_date = datetime.now() - timedelta(days=5*365)  # Five years ago\n",
    "end_date = datetime.now()\n",
    "last_month_start = datetime.now() - timedelta(days=30)  # Define last month's start\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "data = []\n",
    "for date in dates:\n",
    "    num_outages = np.random.randint(0, len(device_df) // 10)  # Random number of outages per day\n",
    "    sampled_devices = device_df.sample(num_outages)\n",
    "    \n",
    "    for _, row in sampled_devices.iterrows():\n",
    "        if date >= last_month_start:\n",
    "            outage_resolved_days = np.random.randint(1, 15)  # Faster resolution in last month\n",
    "        else:\n",
    "            outage_resolved_days = np.random.randint(1, 30)  # Standard resolution time\n",
    "\n",
    "        outage_resolved_date = date + timedelta(days=outage_resolved_days)\n",
    "        \n",
    "        data.append({\n",
    "            'outage_date': date,\n",
    "            'tower_id': row['tower_id'],\n",
    "            'device_id': row['device_id'],\n",
    "            'outage_resolved_date': outage_resolved_date\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "outage_df = pd.DataFrame(data)\n",
    "\n",
    "# Display sample data\n",
    "print(outage_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1e8ef4-d29f-480f-82db-b8e080149b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, col\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "outage_spark_df = spark.createDataFrame(outage_df)\n",
    "\n",
    "# Add date difference column\n",
    "outage_spark_df = outage_spark_df.withColumn(\"date_diff\", datediff(col(\"outage_resolved_date\"), col(\"outage_date\")))\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "outage_spark_df.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.outage_resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff9af05e-a9b9-464c-a860-919bc591c3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6326189844429054,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DAIS Telco SHN Data Simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
