{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefd0788-c5e1-40e8-9452-06d7354b81c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DAIS Demo - Telco Self-Healing Networks Data Simulation\n",
    "\n",
    "This notebook simulates comprehensive telecommunications data for the self-healing networks system. It creates realistic mobile tower locations, device configurations, field technician data, and outage scenarios.\n",
    "\n",
    "## Overview\n",
    "- **Purpose**: Generate comprehensive synthetic data for the entire system\n",
    "- **Data Types**: Tower locations, devices, technicians, outages, scenarios\n",
    "- **Realism**: Based on real-world telecommunications patterns\n",
    "- **Scale**: Simulates multiple years of operational data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3dce02-786a-434a-8e09-04d0c10a2870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Install Dependencies and Load Configuration\n",
    "\n",
    "Install the Faker library for generating realistic synthetic data and load configuration parameters for the data simulation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d106fe07-1ffe-4fd5-ad66-83f9ae8fff31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a17a52f-905c-4739-ae0e-9b80db07cb2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load and Process Mobile Tower Location Data\n",
    "\n",
    "Load mobile tower location data from CSV files and process it for the San Francisco area. This includes:\n",
    "- Loading OpenCellID tower data\n",
    "- Filtering for LTE towers in San Francisco\n",
    "- Adding proper column comments and metadata\n",
    "- Converting timestamps to proper format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea22025-ad1f-43a1-8322-9fbafd18d263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('params.yml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "data_params = params.get('data_params')\n",
    "CATALOG = data_params.get('catalog')\n",
    "SCHEMA = data_params.get('schema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8f4ad5-7013-4fe1-94e3-91f883ab5bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Generate Field Technician Data\n",
    "\n",
    "Generate realistic field technician data including:\n",
    "- Unique technician IDs and names\n",
    "- Truck IDs for each technician\n",
    "- Configurable number of technicians based on simulation parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033a45d4-00fe-41f2-89a2-550c5a2ea998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "colnames = ['radio', 'mcc', 'net', 'area', 'cell', 'unit', 'lon', 'lat', 'range', 'samples', 'changeable', 'created', 'updated', 'averageSignal']\n",
    "\n",
    "mobilelocations = pd.read_csv(f\"/Volumes/{CATALOG}/{SCHEMA}/{data_params.get('towers_volume')}/Tower Locations.csv\", names=colnames)\n",
    "\n",
    "spark.createDataFrame(mobilelocations).write.saveAsTable(f\"{CATALOG}.{SCHEMA}.mobilelocations\", mode=\"overwrite\")\n",
    "for col, comment in zip(colnames, [\n",
    "    \"Network type. One of the strings GSM, UMTS, LTE or CDMA.\",\n",
    "    \"Mobile Country Code, for example 260 for Poland.\",\n",
    "    \"For GSM, UMTS and LTE networks, this is the Mobile Network Code (MNC). For CDMA networks, this is the System IDentification number (SID).\",\n",
    "    \"Location Area Code (LAC) for GSM and UMTS networks. Tracking Area Code (TAC) for LTE networks. Network IDenfitication number (NID) for CDMA networks.\",\n",
    "    \"Cell ID (CID) for GSM and LTE networks. UTRAN Cell ID / LCID for UMTS networks, which is the concatenation of 2 or 4 bytes of Radio Network Controller (RNC) code and 4 bytes of Cell ID. Base station IDentifier number (BID) for CDMA networks.\",\n",
    "    \"Primary Scrambling Code (PSC) for UMTS networks. Physical Cell ID (PCI) for LTE networks. An empty value for GSM and CDMA networks.\",\n",
    "    \"Longitude in degrees between -180.0 and 180.0 changeable=1: average of longitude values of all related measurements changeable=0: exact GPS position of the cell tower\",\n",
    "    \"Latitude in degrees between -90.0 and 90.0 changeable=1: average of latitude values of all related measurements changeable=0: exact GPS position of the tower\",\n",
    "    \"Estimate of cell range, in meters.\",\n",
    "    \"Total number of measurements assigned to the cell tower\",\n",
    "    \"Defines if coordinates of the cell tower are exact or approximate. changeable=1: the GPS position of the cell tower has been calculated from all available measurements changeable=0: the GPS position of the cell tower is precise - no measurements have been used to calculate it.\",\n",
    "    \"The first time when the cell tower was seen and added to the OpenCellID database. A date in timestamp format: number of seconds since the UTC Unix Epoch of 1970-01-01T00:00:00Z For example 1409522613 is the timestamp for 2014-08-31T22:03:33Z.\",\n",
    "    \"The last time when the cell tower was seen and update. A date in timestamp format: number of seconds since the UTC Unix Epoch of 1970-01-01T00:00:00Z For example 1409522613 is the timestamp for 2014-08-31T22:03:33Z.\",\n",
    "    \"Average signal strength.\"\n",
    "]):\n",
    "    spark.sql(f\"ALTER TABLE {CATALOG}.{SCHEMA}.mobilelocations CHANGE COLUMN {col} COMMENT '{comment}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e933cf08-f5fd-42e2-b5cc-75109729ea56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Create Device Inventory\n",
    "\n",
    "Create a comprehensive device inventory for each tower including:\n",
    "- Various types of mobile tower equipment (Nokia, Ericsson, Delta, Cisco, etc.)\n",
    "- Device IDs and tower associations\n",
    "- Realistic device distribution across towers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5610f75-72f0-4eae-a71a-2005a9dd09b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_unixtime\n",
    "\n",
    "mobilelocations = (spark.table(f\"{CATALOG}.{SCHEMA}.mobilelocations\")\n",
    "                    .withColumn(\"created\", from_unixtime(col(\"created\")).cast(\"timestamp\"))\n",
    "                    .withColumn(\"updated\", from_unixtime(col(\"updated\")).cast(\"timestamp\")))\n",
    "\n",
    "\n",
    "sf_mobilelocations = mobilelocations.filter(\n",
    "    (mobilelocations.lat >= 37.6) & (mobilelocations.lat <= 37.9) &\n",
    "    (mobilelocations.lon >= -123.0) & (mobilelocations.lon <= -122.3) &\n",
    "    (mobilelocations.radio == 'LTE')\n",
    ").withColumn(\"tower_id\", expr(\"uuid()\"))\n",
    "\n",
    "\n",
    "sf_mobilelocations.write.option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.sf_mobilelocations\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f48981-a58f-4095-b6b0-9e9b6c129ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Generate Outage Simulation Data\n",
    "\n",
    "Generate realistic outage data over a 5-year period including:\n",
    "- Random outage occurrences per day\n",
    "- Variable resolution times (faster in recent months)\n",
    "- Device-specific outage patterns\n",
    "- Historical outage tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4665ea8c-adfc-45b1-b176-3710866ebd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Field Technician Data Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c96c9a-458d-4816-88aa-3df2ff0ec672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Create Database Tables\n",
    "\n",
    "Create the necessary database tables for the self-healing networks system:\n",
    "- Device status tracking table\n",
    "- Device status triage table  \n",
    "- Triaged devices table\n",
    "- Configure Delta Lake properties for change data feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6dc185-1888-4720-b0b0-5cb72421d4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Generate data for field technicians\n",
    "tech_data = [\n",
    "    Row(tech_id=fake.uuid4(), name=fake.name(), truckid=random.randint(1000, 9999))\n",
    "    for _ in range(params.get('simulation_params').get('n_fieldtechs'))\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of Rows\n",
    "field_technicians = spark.createDataFrame(tech_data)\n",
    "field_technicians.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.field_technicians\")\n",
    "display(field_technicians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e1dbfb-cb2d-47ce-86b9-65350ee0919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devicelist = ['Nokia AirScale Baseband Unit', 'Ericsson RBS 2106', 'Ericsson RRH 6000', 'Ericsson AIR Antenna 5331', 'Delta D750 DC Power System', 'Delta DPR 4000B EnergE', 'Generac MDG25IF4-STD3', 'Swift Sensors SS3-301 Wireless Door Sensor', 'Cisco Meraki MT10 Sensor', 'Cisco Catalyst 2960', 'Cisco ISR 4000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06578864-0010-45ec-a172-924505747a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, explode, expr\n",
    "towers = spark.table(f\"{CATALOG}.{SCHEMA}.sf_mobilelocations\").withColumn('devices', lit(devicelist))\n",
    "\n",
    "towers = towers.withColumn('device', explode('devices')).drop('devices').withColumn('device_id', expr(\"uuid()\"))\n",
    "towers.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.tower_devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40f087e-513f-41d8-baf2-a72e1f52b6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sequence, to_date, expr\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the schema for the simulated data\n",
    "schema = StructType([\n",
    "    StructField(\"tower_id\", IntegerType(), False),\n",
    "    StructField(\"device_name\", StringType(), False),\n",
    "    StructField(\"outage_detected_date\", StringType(), False),\n",
    "    StructField(\"outage_resolved_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.today() - timedelta(days=5*365)).strftime(\"%Y-%m-%d\")\n",
    "date_seq = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date_seq\").selectExpr(\"explode(date_seq) as date\")\n",
    "\n",
    "# Simulate data\n",
    "data = []\n",
    "for row in date_seq.collect():\n",
    "    date = row['date']\n",
    "    for tower_id in range(1, 11):  # Assuming 10 towers\n",
    "        for device_id in range(1, 6):  # Assuming 5 devices per tower\n",
    "            outage_detected_date = date\n",
    "            outage_resolved_date = (datetime.strptime(str(date), \"%Y-%m-%d\") + timedelta(days=random.randint(1, 30))).strftime(\"%Y-%m-%d\")\n",
    "            data.append((tower_id, f\"device_{device_id}\", str(outage_detected_date), str(outage_resolved_date)))\n",
    "\n",
    "# Create DataFrame\n",
    "simulated_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Display the simulated data\n",
    "display(simulated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2562ed66-11b8-416e-800f-1212d38c555e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "towers = spark.table(f\"{CATALOG}.{SCHEMA}.tower_devices\")\n",
    "# tower_ids = towers.select('tower_id').toPandas()['tower_id'].to_list()\n",
    "# device_ids = towers.select('device_id').toPandas()['device_id'].to_list()\n",
    "# num_towers = towers.select('device_id').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create device dataframe\n",
    "device_df = towers.select(['tower_id', 'device_id']).toPandas()\n",
    "\n",
    "# Generate outage data\n",
    "start_date = datetime.now() - timedelta(days=5*365)  # Five years ago\n",
    "end_date = datetime.now()\n",
    "last_month_start = datetime.now() - timedelta(days=30)  # Define last month's start\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "data = []\n",
    "for date in dates:\n",
    "    num_outages = np.random.randint(0, len(device_df) // 10)  # Random number of outages per day\n",
    "    sampled_devices = device_df.sample(num_outages)\n",
    "    \n",
    "    for _, row in sampled_devices.iterrows():\n",
    "        if date >= last_month_start:\n",
    "            outage_resolved_days = np.random.randint(1, 15)  # Faster resolution in last month\n",
    "        else:\n",
    "            outage_resolved_days = np.random.randint(1, 30)  # Standard resolution time\n",
    "\n",
    "        outage_resolved_date = date + timedelta(days=outage_resolved_days)\n",
    "        \n",
    "        data.append({\n",
    "            'outage_date': date,\n",
    "            'tower_id': row['tower_id'],\n",
    "            'device_id': row['device_id'],\n",
    "            'outage_resolved_date': outage_resolved_date\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "outage_df = pd.DataFrame(data)\n",
    "\n",
    "# Display sample data\n",
    "print(outage_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1e8ef4-d29f-480f-82db-b8e080149b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, col\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "outage_spark_df = spark.createDataFrame(outage_df)\n",
    "\n",
    "# Add date difference column\n",
    "outage_spark_df = outage_spark_df.withColumn(\"date_diff\", datediff(col(\"outage_resolved_date\"), col(\"outage_date\")))\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "outage_spark_df.write.mode('overwrite').saveAsTable(f\"{CATALOG}.{SCHEMA}.outage_resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0849e0c-b39f-40ec-9473-aee6af03b589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.device_status (\n",
    "    tower_id STRING,\n",
    "    device_id STRING,\n",
    "    device STRING,\n",
    "    status STRING,\n",
    "    event_timestamp TIMESTAMP,\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "ALTER TABLE {CATALOG}.{SCHEMA}.device_status\n",
    "SET TBLPROPERTIES (\n",
    "    delta.enableChangeDataFeed = true,\n",
    "    delta.appendOnly = true\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9af05e-a9b9-464c-a860-919bc591c3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.device_status_triage (\n",
    "        tower_id STRING,\n",
    "        device_id STRING,\n",
    "        device STRING,\n",
    "        status STRING,\n",
    "        event_timestamp TIMESTAMP,\n",
    "        telemetry_error STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{SCHEMA}.device_status_triage\n",
    "    SET TBLPROPERTIES (\n",
    "        delta.enableChangeDataFeed = true,\n",
    "        delta.appendOnly = true\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c817730-bc39-4a7f-89d7-19709388c0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.triaged_devices (\n",
    "        tower_id STRING,\n",
    "        device_id STRING,\n",
    "        device STRING,\n",
    "        solution STRING,\n",
    "        event_timestamp TIMESTAMP,\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{SCHEMA}.triaged_devices\n",
    "    SET TBLPROPERTIES (\n",
    "        delta.enableChangeDataFeed = true,\n",
    "        delta.appendOnly = true\n",
    "    )\n",
    "\"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6326189844429054,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DAIS Telco SHN Data Simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
