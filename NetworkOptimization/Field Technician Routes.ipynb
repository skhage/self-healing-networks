{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879692b9-d3a3-402c-9f31-b4d307ec1727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('params.yml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "data_params = params.get('data_params')\n",
    "CATALOG = data_params.get('catalog')\n",
    "SCHEMA = data_params.get('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01beb1a5-14d7-4cb3-a3c5-d373148ed927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "from pyspark.sql import Row\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "route_data = []\n",
    "sf_sampled = spark.table(f\"{CATALOG}.{SCHEMA}.sf_mobilelocations\")\n",
    "field_technicians = spark.table(f\"{CATALOG}.{SCHEMA}.field_technicians\")\n",
    "\n",
    "lons = sf_sampled.select('lon').toPandas()['lon'].tolist()\n",
    "lats = sf_sampled.select('lat').toPandas()['lat'].tolist()\n",
    "for tech in field_technicians.collect():\n",
    "    if random.randint(1, 7) <= 2:  # 2/7 chance of being off work\n",
    "        route_data.append(Row(\n",
    "            guid=tech.tech_id,\n",
    "            name=tech.name,\n",
    "            lat=None,\n",
    "            lon=None,\n",
    "            stop_number=None,\n",
    "            random_text=\"not scheduled\",\n",
    "            date=datetime.today().strftime('%Y-%m-%d'),\n",
    "        ))\n",
    "    else:\n",
    "        num_stops = random.randint(1, 15)\n",
    "        base_lat = random.choice(lats)\n",
    "        base_lon = lons[lats.index(base_lat)]\n",
    "        for stop_num in range(1, num_stops + 1):\n",
    "            route_data.append(Row(\n",
    "                guid=tech.tech_id,\n",
    "                name=tech.name,\n",
    "                lat=base_lat + random.uniform(-0.01, 0.01),\n",
    "                lon=base_lon + random.uniform(-0.01, 0.01),\n",
    "                stop_number=stop_num,\n",
    "                random_text=fake.text(max_nb_chars=50),\n",
    "                date=datetime.today().strftime('%Y-%m-%d'),\n",
    "            ))\n",
    "\n",
    "routes_df = spark.createDataFrame(route_data)\n",
    "routes_df.write.mode(\"append\").saveAsTable(f\"{CATALOG}.{SCHEMA}.fieldtech_route\")\n",
    "\n",
    "display(routes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4f6d886-c92e-4bf9-b9a7-ca0f8ab85b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "pyyaml",
     "faker"
    ],
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Field Technician Routes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
